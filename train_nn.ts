/**
 * @file Main Training Script (Node.js, Memory Optimized)
 * This script trains the dual-head ResNet model using data generated by self-play.
 * It processes data in chunks to keep memory usage low.
 */

import * as tf from '@tensorflow/tfjs-node-gpu';
import { readFileSync } from 'node:fs';
import * as path from 'path';
import { createDualResNetModel } from './src/model';
import type { Player } from './src/ai';

// --- Configuration ---
const DATA_FILE_PATH = './training_data.jsonl';
const MODEL_SAVE_PATH = './gomoku_model';
const BOARD_SIZE = 19;

// --- Training Hyperparameters ---
const EPOCHS = 10;
const CHUNK_SIZE = 8192; // Number of original samples to process at a time
const BATCH_SIZE = 128;

interface TrainingSample {
    state: (Player | null)[][];
    policy: number[];
    value: number;
    player: Player;
}

// --- Data Processing and Augmentation ---

function parseLinesToSamples(lines: string[]): TrainingSample[] {
    return lines.filter(line => line).map(line => JSON.parse(line));
}

function augmentAndConvertToTensors(samples: TrainingSample[]): { xs: tf.Tensor4D, ys: { policy: tf.Tensor2D, value: tf.Tensor2D } } {
    return tf.tidy(() => {
        const augmentedStates: tf.Tensor4D[] = [];
        const augmentedPolicies: tf.Tensor2D[] = [];
        const augmentedValues: number[][] = [];

        for (const sample of samples) {
            const symmetries = getSymmetries(sample.state, sample.policy);
            for (const sym of symmetries) {
                const player = sample.player || 'black';
                const opponent: Player = player === 'black' ? 'white' : 'black';

                const playerChannel = Array(BOARD_SIZE).fill(0).map(() => Array(BOARD_SIZE).fill(0));
                const opponentChannel = Array(BOARD_SIZE).fill(0).map(() => Array(BOARD_SIZE).fill(0));
                for (let r = 0; r < BOARD_SIZE; r++) {
                    for (let c = 0; c < BOARD_SIZE; c++) {
                        if (sym.state[r][c] === player) playerChannel[r][c] = 1;
                        else if (sym.state[r][c] === opponent) opponentChannel[r][c] = 1;
                    }
                }
                const colorChannel = Array(BOARD_SIZE).fill(0).map(() => Array(BOARD_SIZE).fill(player === 'black' ? 1 : 0));

                const stacked = tf.stack([tf.tensor2d(playerChannel), tf.tensor2d(opponentChannel), tf.tensor2d(colorChannel)], 2);
                augmentedStates.push(stacked.expandDims(0) as tf.Tensor4D);
                augmentedPolicies.push(tf.tensor2d([sym.policy]));
                augmentedValues.push([sample.value]);
            }
        }

        return {
            xs: tf.concat(augmentedStates),
            ys: {
                policy: tf.concat(augmentedPolicies),
                value: tf.tensor2d(augmentedValues)
            }
        };
    });
}

function getSymmetries(state: (Player | null)[][], policy: number[]): { state: (Player | null)[][], policy: number[] }[] {
    const symmetries = [];
    let currentBoard = state.map(row => [...row]);
    let currentPolicy = [...policy];
    for (let i = 0; i < 4; i++) {
        symmetries.push({ state: currentBoard, policy: currentPolicy });
        symmetries.push({ state: flipBoard(currentBoard), policy: flipPolicy(currentPolicy) });
        currentBoard = rotateBoard(currentBoard);
        currentPolicy = rotatePolicy(currentPolicy);
    }
    return symmetries;
}

function rotateBoard(board: (Player | null)[][]): (Player | null)[][] {
    const newBoard = Array(BOARD_SIZE).fill(null).map(() => Array(BOARD_SIZE).fill(null));
    for (let r = 0; r < BOARD_SIZE; r++) {
        for (let c = 0; c < BOARD_SIZE; c++) { newBoard[c][BOARD_SIZE - 1 - r] = board[r][c]; }
    }
    return newBoard;
}

function flipBoard(board: (Player | null)[][]): (Player | null)[][] {
    return board.map(row => row.slice().reverse());
}

function rotatePolicy(policy: number[]): number[] {
    const newPolicy = Array(BOARD_SIZE * BOARD_SIZE).fill(0);
    for (let r = 0; r < BOARD_SIZE; r++) {
        for (let c = 0; c < BOARD_SIZE; c++) { newPolicy[c * BOARD_SIZE + (BOARD_SIZE - 1 - r)] = policy[r * BOARD_SIZE + c]; }
    }
    return newPolicy;
}

function flipPolicy(policy: number[]): number[] {
    const newPolicy = Array(BOARD_SIZE * BOARD_SIZE).fill(0);
    for (let r = 0; r < BOARD_SIZE; r++) {
        for (let c = 0; c < BOARD_SIZE; c++) { newPolicy[r * BOARD_SIZE + (BOARD_SIZE - 1 - c)] = policy[r * BOARD_SIZE + c]; }
    }
    return newPolicy;
}

// --- Main Training Logic ---

async function train() {
    console.log(`Loading training data lines from ${DATA_FILE_PATH}...`);
    let allLines: string[];
    try {
        const fileContent = readFileSync(DATA_FILE_PATH, 'utf-8');
        allLines = fileContent.trim().split('\n');
        if (allLines.length === 0 || (allLines.length === 1 && !allLines[0])) {
            console.error('Training data file is empty.');
            return;
        }
        console.log(`Loaded ${allLines.length} data lines.`);
    } catch (error) {
        console.error(`Error reading training data file: ${error}`);
        return;
    }

    let model;
    try {
        model = await tf.loadLayersModel(`file://${MODEL_SAVE_PATH}/model.json`);
        console.log('Existing model loaded.');
    } catch (error) {
        console.log('No existing model found. Creating a new model...');
        model = createDualResNetModel();
    }

    model.compile({
        optimizer: tf.train.adam(),
        loss: { 'policy': 'categoricalCrossentropy', 'value': 'meanSquaredError' },
        metrics: { 'policy': 'accuracy', 'value': tf.metrics.meanAbsoluteError }
    });

    const NUM_CHUNKS = Math.ceil(allLines.length / CHUNK_SIZE);

    for (let epoch = 0; epoch < EPOCHS; epoch++) {
        console.log(`\n--- Epoch ${epoch + 1} / ${EPOCHS} ---\
`);
        tf.util.shuffle(allLines);

        for (let i = 0; i < NUM_CHUNKS; i++) {
            const start = i * CHUNK_SIZE;
            const end = Math.min(start + CHUNK_SIZE, allLines.length);
            const lineChunk = allLines.slice(start, end);

            console.log(`\nProcessing chunk ${i + 1}/${NUM_CHUNKS} (lines ${start}-${end})
`);
            const samples = parseLinesToSamples(lineChunk);
            if (samples.length === 0) continue;

            const { xs, ys } = augmentAndConvertToTensors(samples);

            await model.fit(xs, ys, {
                batchSize: BATCH_SIZE,
                epochs: 1,
                shuffle: true,
            });

            xs.dispose();
            ys.policy.dispose();
            ys.value.dispose();
            console.log(`Memory freed for chunk ${i + 1}. Num Tensors: ${tf.memory().numTensors}`);
        }
    }

    console.log('\nModel training finished.');
    console.log(`Saving model to ${MODEL_SAVE_PATH}...`);
    await model.save(`file://${path.resolve(MODEL_SAVE_PATH)}`);
    console.log('Model saved successfully.');
}

train().catch(console.error);
